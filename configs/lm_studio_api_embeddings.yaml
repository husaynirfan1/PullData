# LM Studio Configuration with API Embeddings
#
# This configuration uses LM Studio for BOTH embeddings and LLM.
#
# Prerequisites:
# 1. Download LM Studio from https://lmstudio.ai/
# 2. Download an embedding model (e.g., nomic-embed-text-v1.5)
# 3. Download an LLM (e.g., Qwen2.5-3B-Instruct)
# 4. Load the embedding model in LM Studio
# 5. Start the LM Studio local server
# 
# Note: You can run different embedding models on different ports,
# or use the same LM Studio instance for both if it supports it.

storage:
  backend: local
  local:
    sqlite_path: ./data/pulldata.db
    faiss_index_path: ./data/faiss_indexes
    create_if_missing: true

models:
  # Embedding Model (via LM Studio API)
  embedder:
    provider: api  # Use 'api' for LM Studio embeddings
    dimension: 768  # Adjust based on your embedding model
    
    api:
      base_url: http://localhost:1234/v1  # LM Studio URL
      api_key: lm-studio
      model: nomic-embed-text-v1.5  # Your loaded embedding model name
      timeout: 60
      max_retries: 3
      batch_size: 100  # Process 100 texts per API call

  # LLM (via LM Studio API)
  llm:
    provider: api
    
    api:
      base_url: http://localhost:1234/v1
      api_key: lm-studio
      model: local-model  # Or specific LLM model name
      timeout: 60
      max_retries: 3

    generation:
      max_tokens: 2048
      temperature: 0.7
      top_p: 0.9

# Rest of config same as default
parsing:
  pdf:
    backend: pymupdf
    extract_images: false
    extract_tables: true
    
  chunking:
    strategy: semantic
    chunk_size: 512
    chunk_overlap: 50
    min_chunk_size: 100
    respect_sentence_boundary: true

retrieval:
  vector_search:
    index_type: flat
    metric: cosine
    top_k: 5

cache:
  llm_output:
    enabled: true
    ttl_hours: 24
    max_entries: 10000

  embedding:
    enabled: true
    ttl_hours: 168
    max_entries: 50000

logging:
  level: INFO
  log_to_file: true
  log_file: ./logs/pulldata.log

performance:
  batch_size: 32
  num_workers: 4
  show_progress: true

features:
  differential_updates: true
  llm_caching: true
