# LM Studio Configuration for PullData
# 
# Use this configuration when running LLM via LM Studio
# and embeddings locally with sentence-transformers.
#
# Prerequisites:
# 1. Download and install LM Studio from https://lmstudio.ai/
# 2. Download a model in LM Studio (e.g., Qwen2.5-3B-Instruct, Llama-3.2-3B)
# 3. Start the LM Studio local server (Server tab)
# 4. Default URL: http://localhost:1234/v1

# Storage Backend
storage:
  backend: local  # Use local SQLite + FAISS

  local:
    sqlite_path: ./data/pulldata.db
    faiss_index_path: ./data/faiss_indexes
    create_if_missing: true

# Model Configuration
models:
  # Embedding Model (runs locally, NOT via LM Studio)
  embedder:
    name: BAAI/bge-small-en-v1.5  # Recommended: small, fast, good quality
    dimension: 384
    batch_size: 32
    device: cpu  # Change to 'cuda' if you have GPU
    normalize_embeddings: true
    cache_dir: ./models/embeddings

  # LLM Configuration (via LM Studio API)
  llm:
    provider: api  # Use API provider for LM Studio
    
    # LM Studio API settings
    api:
      base_url: http://localhost:1234/v1  # LM Studio default URL
      api_key: lm-studio  # LM Studio doesn't require a real API key
      model: local-model  # LM Studio uses this generic name
      timeout: 60  # Request timeout in seconds
      max_retries: 3

    # Generation parameters (adjust to your preference)
    generation:
      max_tokens: 2048  # Maximum response length
      temperature: 0.7  # Randomness (0.0 = deterministic, 2.0 = very random)
      top_p: 0.9  # Nucleus sampling
      top_k: 50  # Top-k sampling
      frequency_penalty: 0.0
      presence_penalty: 0.0

# Document Processing
parsing:
  pdf:
    backend: pymupdf
    extract_images: false
    extract_tables: true
    
  chunking:
    strategy: semantic  # Options: 'semantic', 'fixed', 'sentence'
    chunk_size: 512  # In tokens
    chunk_overlap: 50
    min_chunk_size: 100
    respect_sentence_boundary: true

  hashing:
    algorithm: sha256
    enabled: true

# Retrieval Configuration
retrieval:
  vector_search:
    index_type: flat  # Options: 'flat', 'ivf', 'hnsw'
    metric: cosine
    top_k: 5  # Number of chunks to retrieve
    
  filters:
    enabled: true
    default_filters: {}

  reranking:
    enabled: false  # Set to true for better results (slower)
    model: BAAI/bge-reranker-base
    top_n: 3

# Cache Configuration
cache:
  llm_output:
    enabled: true  # Cache LLM responses
    ttl_hours: 24
    max_entries: 10000
    eviction_policy: lru

  embedding:
    enabled: true  # Cache embeddings
    ttl_hours: 168  # 7 days
    max_entries: 50000

# Project Settings
project:
  name: default_project
  description: "PullData project using LM Studio"
  metadata:
    tags: []

# Output Generation
output:
  excel:
    engine: openpyxl
    default_sheet_name: Sheet1
    include_metadata: true
    
  markdown:
    include_toc: true
    code_theme: github
    heading_level: 2
    
  json:
    indent: 2
    ensure_ascii: false
    sort_keys: false

# Logging
logging:
  level: INFO  # Options: DEBUG, INFO, WARNING, ERROR
  log_to_file: true
  log_file: ./logs/pulldata.log
  rotation: 10 MB
  retention: 30 days

# Performance
performance:
  batch_size: 32
  num_workers: 4
  max_memory_gb: 6
  clear_cache_after_ingest: false
  show_progress: true

# Feature Flags
features:
  differential_updates: true  # Skip unchanged documents
  multi_project: true
  advanced_filtering: true
  llm_caching: true

# Security
security:
  validate_file_types: true
  allowed_extensions: [.pdf, .docx, .txt]
  max_file_size_mb: 100
  sanitize_inputs: true
